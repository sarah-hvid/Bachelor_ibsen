{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script used the evaluation CSV files to calculate the results for each NE type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import eval4ner.muc as muc\n",
    "import pprint\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c0e5f8d30a91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\misc\\\\dacy_small_w_annotation.csv', converters={'dacy_small': literal_eval, 'ground_truths': literal_eval})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mcsv_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\all_except_misc\\\\dacy_small_w_annotation.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'dacy_small'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ground_truths'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\person\\\\dacy_small_w_annotation.csv', converters={'dacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\location\\\\dacy_small_w_annotation.csv', converters={'dacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\organization\\\\dacy_small_w_annotation.csv', converters={'dacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\misc\\\\dacy_small_w_annotation.csv', converters={'dacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\all_except_misc\\\\dacy_small_w_annotation.csv', converters={'dacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "\n",
    "for i,r in csv_df.iterrows():\n",
    "    model = r['dacy_small']\n",
    "    predictions.append(model)\n",
    "    \n",
    "    ground_truth = r['ground_truths']\n",
    "    ground_truths.append(ground_truth)\n",
    "    \n",
    "    text = r['texts']\n",
    "    texts.append(text)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "ground_truths_r = ground_truths\n",
    "\n",
    "#dacy_s_result = muc.evaluate_all(predictions, ground_truths_r * 1, texts, verbose=True)\n",
    "pprint.pprint(dacy_s_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n",
      "219\n",
      "{'exact': {'count': 120,\n",
      "           'f1_score': 78.20680706391825,\n",
      "           'precision': 85.27523809523808,\n",
      "           'recall': 75.81492557150452},\n",
      " 'partial': {'count': 120,\n",
      "             'f1_score': 79.32826795596738,\n",
      "             'precision': 86.42999999999998,\n",
      "             'recall': 76.93635414293308},\n",
      " 'strict': {'count': 120,\n",
      "            'f1_score': 78.20680706391825,\n",
      "            'precision': 85.27523809523808,\n",
      "            'recall': 75.81492557150452},\n",
      " 'type': {'count': 120,\n",
      "          'f1_score': 80.44972884801649,\n",
      "          'precision': 87.5847619047619,\n",
      "          'recall': 78.05778271436165}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\person\\\\dacy_medium_w_annotation.csv', converters={'dacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\location\\\\dacy_medium_w_annotation.csv', converters={'dacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\organization\\\\dacy_medium_w_annotation.csv', converters={'dacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\misc\\\\dacy_medium_w_annotation.csv', converters={'dacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\all_except_misc\\\\dacy_medium_w_annotation.csv', converters={'dacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "\n",
    "for i,r in csv_df.iterrows():\n",
    "    model = r['dacy_medium']\n",
    "    predictions.append(model)\n",
    "    \n",
    "    ground_truth = r['ground_truths']\n",
    "    ground_truths.append(ground_truth)\n",
    "    \n",
    "    text = r['texts']\n",
    "    texts.append(text)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "ground_truths_r = ground_truths\n",
    "\n",
    "#result = muc.evaluate_all(predictions, ground_truths_r * 1, texts, verbose=True)\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n",
      "341\n",
      "{'exact': {'count': 120,\n",
      "           'f1_score': 85.5464097818453,\n",
      "           'precision': 86.9519725888147,\n",
      "           'recall': 85.97452781136991},\n",
      " 'partial': {'count': 120,\n",
      "             'f1_score': 92.86433563107919,\n",
      "             'precision': 94.15085879033245,\n",
      "             'recall': 93.64569137003349},\n",
      " 'strict': {'count': 120,\n",
      "            'f1_score': 85.5464097818453,\n",
      "            'precision': 86.9519725888147,\n",
      "            'recall': 85.97452781136991},\n",
      " 'type': {'count': 120,\n",
      "          'f1_score': 100.1822614803131,\n",
      "          'precision': 101.34974499185022,\n",
      "          'recall': 101.31685492869704}}\n"
     ]
    }
   ],
   "source": [
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\person\\\\dacy_large_w_annotation.csv', converters={'dacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\location\\\\dacy_large_w_annotation.csv', converters={'dacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\organization\\\\dacy_large_w_annotation.csv', converters={'dacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\misc\\\\dacy_large_w_annotation.csv', converters={'dacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\all_except_misc\\\\dacy_large_w_annotation.csv', converters={'dacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "\n",
    "for i,r in csv_df.iterrows():\n",
    "    model = r['dacy_large']\n",
    "    predictions.append(model)\n",
    "    \n",
    "    ground_truth = r['ground_truths']\n",
    "    ground_truths.append(ground_truth)\n",
    "    \n",
    "    text = r['texts']\n",
    "    texts.append(text)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "ground_truths_r = ground_truths\n",
    "\n",
    "#result = muc.evaluate_all(predictions, ground_truths_r * 1, texts, verbose=True)\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n",
      "233\n",
      "{'exact': {'count': 120,\n",
      "           'f1_score': 81.37619266968123,\n",
      "           'precision': 89.13322510822509,\n",
      "           'recall': 78.56710811118704},\n",
      " 'partial': {'count': 120,\n",
      "             'f1_score': 82.39176223083903,\n",
      "             'precision': 90.15882034632034,\n",
      "             'recall': 79.60252477785369},\n",
      " 'strict': {'count': 120,\n",
      "            'f1_score': 81.37619266968123,\n",
      "            'precision': 89.13322510822509,\n",
      "            'recall': 78.56710811118704},\n",
      " 'type': {'count': 120,\n",
      "          'f1_score': 83.40733179199682,\n",
      "          'precision': 91.18441558441558,\n",
      "          'recall': 80.63794144452038}}\n"
     ]
    }
   ],
   "source": [
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\person\\\\spacy_trf_w_annotation.csv', converters={'spacy_trf': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\location\\\\spacy_trf_w_annotation.csv', converters={'spacy_trf': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\organization\\\\spacy_trf_w_annotation.csv', converters={'spacy_trf': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\misc\\\\spacy_trf_w_annotation.csv', converters={'spacy_trf': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\all_except_misc\\\\spacy_trf_w_annotation.csv', converters={'spacy_trf': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "\n",
    "for i,r in csv_df.iterrows():\n",
    "    model = r['spacy_trf']\n",
    "    predictions.append(model)\n",
    "    \n",
    "    ground_truth = r['ground_truths']\n",
    "    ground_truths.append(ground_truth)\n",
    "    \n",
    "    text = r['texts']\n",
    "    texts.append(text)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "ground_truths_r = ground_truths\n",
    "\n",
    "#result = muc.evaluate_all(predictions, ground_truths_r * 1, texts, verbose=True)\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387\n",
      "684\n",
      "{'exact': {'count': 120,\n",
      "           'f1_score': 78.91364642204077,\n",
      "           'precision': 76.5309380193127,\n",
      "           'recall': 88.87763315116256},\n",
      " 'partial': {'count': 120,\n",
      "             'f1_score': 79.02881915780175,\n",
      "             'precision': 76.64592719680188,\n",
      "             'recall': 89.00880198233138},\n",
      " 'strict': {'count': 120,\n",
      "            'f1_score': 78.81840832680268,\n",
      "            'precision': 76.4684380193127,\n",
      "            'recall': 88.67763315116255},\n",
      " 'type': {'count': 120,\n",
      "          'f1_score': 79.04875379832463,\n",
      "          'precision': 76.69841637429106,\n",
      "          'recall': 88.93997081350021}}\n"
     ]
    }
   ],
   "source": [
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\person\\\\spacy_small_w_annotation.csv', converters={'spacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\location\\\\spacy_small_w_annotation.csv', converters={'spacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\organization\\\\spacy_small_w_annotation.csv', converters={'spacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\misc\\\\spacy_small_w_annotation.csv', converters={'spacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\all_except_misc\\\\spacy_small_w_annotation.csv', converters={'spacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "\n",
    "for i,r in csv_df.iterrows():\n",
    "    model = r['spacy_small']\n",
    "    predictions.append(model)\n",
    "    \n",
    "    ground_truth = r['ground_truths']\n",
    "    ground_truths.append(ground_truth)\n",
    "    \n",
    "    text = r['texts']\n",
    "    texts.append(text)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "ground_truths_r = ground_truths\n",
    "\n",
    "#result = muc.evaluate_all(predictions, ground_truths_r * 1, texts, verbose=True)\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n",
      "290\n",
      "{'exact': {'count': 120,\n",
      "           'f1_score': 61.41459324988736,\n",
      "           'precision': 65.99716274716275,\n",
      "           'recall': 61.28760822510822},\n",
      " 'partial': {'count': 120,\n",
      "             'f1_score': 62.12514413690883,\n",
      "             'precision': 66.91593541593542,\n",
      "             'recall': 61.94521815903394},\n",
      " 'strict': {'count': 120,\n",
      "            'f1_score': 61.41459324988736,\n",
      "            'precision': 65.99716274716275,\n",
      "            'recall': 61.28760822510822},\n",
      " 'type': {'count': 120,\n",
      "          'f1_score': 62.835695023930306,\n",
      "          'precision': 67.83470808470808,\n",
      "          'recall': 62.60282809295967}}\n"
     ]
    }
   ],
   "source": [
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\person\\\\spacy_medium_w_annotation.csv', converters={'spacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\location\\\\spacy_medium_w_annotation.csv', converters={'spacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\organization\\\\spacy_medium_w_annotation.csv', converters={'spacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\misc\\\\spacy_medium_w_annotation.csv', converters={'spacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\all_except_misc\\\\spacy_medium_w_annotation.csv', converters={'spacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "\n",
    "for i,r in csv_df.iterrows():\n",
    "    model = r['spacy_medium']\n",
    "    predictions.append(model)\n",
    "    \n",
    "    ground_truth = r['ground_truths']\n",
    "    ground_truths.append(ground_truth)\n",
    "    \n",
    "    text = r['texts']\n",
    "    texts.append(text)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "ground_truths_r = ground_truths\n",
    "\n",
    "#result = muc.evaluate_all(predictions, ground_truths_r * 1, texts, verbose=True)\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n",
      "264\n",
      "{'exact': {'count': 120,\n",
      "           'f1_score': 62.96912323142974,\n",
      "           'precision': 67.50992063492063,\n",
      "           'recall': 62.60284992784994},\n",
      " 'partial': {'count': 120,\n",
      "             'f1_score': 63.6279467608415,\n",
      "             'precision': 68.4015873015873,\n",
      "             'recall': 63.49830447330448},\n",
      " 'strict': {'count': 120,\n",
      "            'f1_score': 62.96912323142974,\n",
      "            'precision': 67.50992063492063,\n",
      "            'recall': 62.60284992784994},\n",
      " 'type': {'count': 120,\n",
      "          'f1_score': 64.28677029025326,\n",
      "          'precision': 69.29325396825396,\n",
      "          'recall': 64.39375901875903}}\n"
     ]
    }
   ],
   "source": [
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\person\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\location\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\organization\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\misc\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\all_except_misc\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "\n",
    "for i,r in csv_df.iterrows():\n",
    "    model = r['spacy_large']\n",
    "    predictions.append(model)\n",
    "    \n",
    "    ground_truth = r['ground_truths']\n",
    "    ground_truths.append(ground_truth)\n",
    "    \n",
    "    text = r['texts']\n",
    "    texts.append(text)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "ground_truths_r = ground_truths\n",
    "\n",
    "#result = muc.evaluate_all(predictions, ground_truths_r * 1, texts, verbose=True)\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done' # all\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1844-1871'\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1871-1879'\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1880-1889'\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1890-1905'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\misc\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\person\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\location\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\organization\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\all_except_misc\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "entities = []\n",
    "\n",
    "for filename in all_files: \n",
    "    brev_filename = filename\n",
    "    brev_filename = re.match(r'.*\\\\(.*).csv', brev_filename)\n",
    "    brev_filename = brev_filename.group(1)\n",
    "    \n",
    "    for i,r in csv_df.iterrows():\n",
    "        files = r['filename']\n",
    "        if files == brev_filename:\n",
    "            annotations = r['ground_truths']\n",
    "            entities.append(annotations)\n",
    "            \n",
    "flat_list = [item for sublist in entities for item in sublist]\n",
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done' # all\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1844-1871'\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1871-1879'\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1880-1889'\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1890-1905'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\misc\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\person\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\location\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\all_except_misc\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "entities = []\n",
    "\n",
    "for filename in all_files: \n",
    "    brev_filename = filename\n",
    "    brev_filename = re.match(r'.*\\\\(.*).csv', brev_filename)\n",
    "    brev_filename = brev_filename.group(1)\n",
    "    \n",
    "    for i,r in csv_df.iterrows():\n",
    "        files = r['filename']\n",
    "        if files == brev_filename:\n",
    "            annotations = r['ground_truths']\n",
    "            annotations = str(annotations)\n",
    "            #print(annotations)\n",
    "            if re.match(r'\\[\\]', annotations):\n",
    "                pass\n",
    "            else:\n",
    "                \n",
    "                anno_word = re.findall(r\"\\[?\\('.*?', '(.*?)'\\),? ?\", annotations)\n",
    "                #print(anno_word)\n",
    "                #anno_word = anno_word.group(1)\n",
    "                entities.append(anno_word)\n",
    "            \n",
    "flat_list = [item for sublist in entities for item in sublist]\n",
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flat_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-44204d1d9dbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mend_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'flat_list' is not defined"
     ]
    }
   ],
   "source": [
    "end_list = str(flat_list).split()\n",
    "len(end_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
