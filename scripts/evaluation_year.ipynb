{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script uses the evaluation CSV files to calculate the results for each time-period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import eval4ner.muc as muc\n",
    "import pprint\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\dacy_medium_w_annotation.csv', converters={'dacy_medium': literal_eval, 'ground_truths': literal_eval})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n",
      "251\n",
      "{'exact': {'count': 30,\n",
      "           'f1_score': 21.009997513067383,\n",
      "           'precision': 21.435062087035774,\n",
      "           'recall': 20.917988994357838},\n",
      " 'partial': {'count': 30,\n",
      "             'f1_score': 23.402420891849594,\n",
      "             'precision': 23.783155331510592,\n",
      "             'recall': 23.3982429880297},\n",
      " 'strict': {'count': 30,\n",
      "            'f1_score': 18.19351090059273,\n",
      "            'precision': 18.21435691501481,\n",
      "            'recall': 18.30861051088131},\n",
      " 'type': {'count': 30,\n",
      "          'f1_score': 22.524410289736107,\n",
      "          'precision': 22.510383914331282,\n",
      "          'recall': 22.727805538406134}}\n"
     ]
    }
   ],
   "source": [
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1844-1871' # use your path\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1871-1879'\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1880-1889'\n",
    "path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1890-1905'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work_misc\\\\csv\\\\dacy_medium_w_annotation.csv', converters={'dacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\dacy_medium_w_annotation.csv', converters={'dacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "file_names = []\n",
    "\n",
    "for filename in all_files: \n",
    "    brev_filename = filename\n",
    "    brev_filename = re.match(r'.*\\d{4}-\\d{4}\\\\(.*).csv', brev_filename)\n",
    "    brev_filename = brev_filename.group(1)\n",
    "    \n",
    "    for i,r in csv_df.iterrows():\n",
    "        files = r['filename']\n",
    "        if files == brev_filename:\n",
    "            model = r['dacy_medium']\n",
    "            predictions.append(model)\n",
    "            f = r['filename']\n",
    "            file_names.append(f)\n",
    "            t = r['texts']\n",
    "            texts.append(t)\n",
    "            g = r['ground_truths']\n",
    "            ground_truths.append(g)\n",
    "    \n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "\n",
    "#x = muc.evaluate_all(predictions, ground_truths * 1, texts, verbose=True)\n",
    "pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n",
      "313\n",
      "{'exact': {'count': 30,\n",
      "           'f1_score': 22.612607728422947,\n",
      "           'precision': 22.314300665364904,\n",
      "           'recall': 23.474987673852272},\n",
      " 'partial': {'count': 30,\n",
      "             'f1_score': 24.11468328471046,\n",
      "             'precision': 23.694111866161013,\n",
      "             'recall': 25.215018446348026},\n",
      " 'strict': {'count': 30,\n",
      "            'f1_score': 20.8902445260054,\n",
      "            'precision': 20.657394012076672,\n",
      "            'recall': 21.54569956783454},\n",
      " 'type': {'count': 30,\n",
      "          'f1_score': 22.552930260158526,\n",
      "          'precision': 22.15139782173451,\n",
      "          'recall': 23.54111199001904}}\n"
     ]
    }
   ],
   "source": [
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1844-1871' # use your path\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1871-1879'\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1880-1889'\n",
    "path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1890-1905'\n",
    "\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work_misc\\\\csv\\\\dacy_small_w_annotation.csv', converters={'dacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\dacy_small_w_annotation.csv', converters={'dacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "file_names = []\n",
    "\n",
    "for filename in all_files: \n",
    "    brev_filename = filename\n",
    "    brev_filename = re.match(r'.*\\d{4}-\\d{4}\\\\(.*).csv', brev_filename)\n",
    "    brev_filename = brev_filename.group(1)\n",
    "    \n",
    "    for i,r in csv_df.iterrows():\n",
    "        files = r['filename']\n",
    "        if files == brev_filename:\n",
    "            model = r['dacy_small']\n",
    "            predictions.append(model)\n",
    "            f = r['filename']\n",
    "            file_names.append(f)\n",
    "            t = r['texts']\n",
    "            texts.append(t)\n",
    "            g = r['ground_truths']\n",
    "            ground_truths.append(g)\n",
    "    \n",
    "\n",
    "lists = {'filename': file_names, 'texts': texts, 'ground_truths': ground_truths, 'dacy_small': predictions}\n",
    "\n",
    "df_dm = pd.DataFrame.from_dict(lists, orient = 'index')\n",
    "df_dm = df_dm.transpose()\n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "#x = muc.evaluate_all(predictions, ground_truths * 1, texts, verbose=True)\n",
    "pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n",
      "264\n",
      "{'exact': {'count': 30,\n",
      "           'f1_score': 23.18891809524108,\n",
      "           'precision': 23.188320707070705,\n",
      "           'recall': 23.34565358594433},\n",
      " 'partial': {'count': 30,\n",
      "             'f1_score': 24.738013992883488,\n",
      "             'precision': 24.69198232323232,\n",
      "             'recall': 24.952703231380013},\n",
      " 'strict': {'count': 30,\n",
      "            'f1_score': 22.71494245847474,\n",
      "            'precision': 22.67790404040404,\n",
      "            'recall': 22.893616234501206},\n",
      " 'type': {'count': 30,\n",
      "          'f1_score': 25.073969348896984,\n",
      "          'precision': 24.992929292929293,\n",
      "          'recall': 25.30993508943424}}\n"
     ]
    }
   ],
   "source": [
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1844-1871' # use your path\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1871-1879'\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1880-1889'\n",
    "path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1890-1905'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work_misc\\\\csv\\\\dacy_large_w_annotation.csv', converters={'dacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\dacy_large_w_annotation.csv', converters={'dacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "file_names = []\n",
    "\n",
    "for filename in all_files: \n",
    "    brev_filename = filename\n",
    "    brev_filename = re.match(r'.*\\d{4}-\\d{4}\\\\(.*).csv', brev_filename)\n",
    "    brev_filename = brev_filename.group(1)\n",
    "    \n",
    "    for i,r in csv_df.iterrows():\n",
    "        files = r['filename']\n",
    "        if files == brev_filename:\n",
    "            model = r['dacy_large']\n",
    "            predictions.append(model)\n",
    "            f = r['filename']\n",
    "            file_names.append(f)\n",
    "            t = r['texts']\n",
    "            texts.append(t)\n",
    "            g = r['ground_truths']\n",
    "            ground_truths.append(g)\n",
    "    \n",
    "\n",
    "lists = {'filename': file_names, 'texts': texts, 'ground_truths': ground_truths, 'dacy_large': predictions}\n",
    "\n",
    "df_dm = pd.DataFrame.from_dict(lists, orient = 'index')\n",
    "df_dm = df_dm.transpose()\n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "#x = muc.evaluate_all(predictions, ground_truths * 1, texts, verbose=True)\n",
    "pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n",
      "567\n",
      "{'exact': {'count': 30,\n",
      "           'f1_score': 18.24528254650084,\n",
      "           'precision': 19.21556598615422,\n",
      "           'recall': 19.139682371250704},\n",
      " 'partial': {'count': 30,\n",
      "             'f1_score': 19.292657431562535,\n",
      "             'precision': 20.545492111668583,\n",
      "             'recall': 20.326187865497076},\n",
      " 'strict': {'count': 30,\n",
      "            'f1_score': 13.972134622089385,\n",
      "            'precision': 14.68687430746254,\n",
      "            'recall': 14.670384875923926},\n",
      " 'type': {'count': 30,\n",
      "          'f1_score': 15.67499176494653,\n",
      "          'precision': 17.00125339242986,\n",
      "          'recall': 16.26205154259059}}\n"
     ]
    }
   ],
   "source": [
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1844-1871' # use your path\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1871-1879'\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1880-1889'\n",
    "path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1890-1905'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work_misc\\\\csv\\\\spacy_small_w_annotation.csv', converters={'spacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\spacy_small_w_annotation.csv', converters={'spacy_small': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "file_names = []\n",
    "\n",
    "for filename in all_files: \n",
    "    brev_filename = filename\n",
    "    brev_filename = re.match(r'.*\\d{4}-\\d{4}\\\\(.*).csv', brev_filename)\n",
    "    brev_filename = brev_filename.group(1)\n",
    "    \n",
    "    for i,r in csv_df.iterrows():\n",
    "        files = r['filename']\n",
    "        if files == brev_filename:\n",
    "            model = r['spacy_small']\n",
    "            predictions.append(model)\n",
    "            f = r['filename']\n",
    "            file_names.append(f)\n",
    "            t = r['texts']\n",
    "            texts.append(t)\n",
    "            g = r['ground_truths']\n",
    "            ground_truths.append(g)\n",
    "    \n",
    "\n",
    "lists = {'filename': file_names, 'texts': texts, 'ground_truths': ground_truths, 'spacy_small': predictions}\n",
    "\n",
    "df_dm = pd.DataFrame.from_dict(lists, orient = 'index')\n",
    "df_dm = df_dm.transpose()\n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "#x = muc.evaluate_all(predictions, ground_truths * 1, texts, verbose=True)\n",
    "pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n",
      "392\n",
      "{'exact': {'count': 30,\n",
      "           'f1_score': 22.36309785752006,\n",
      "           'precision': 22.90452635599695,\n",
      "           'recall': 22.158591646516097},\n",
      " 'partial': {'count': 30,\n",
      "             'f1_score': 24.641102914057722,\n",
      "             'precision': 25.158893557422967,\n",
      "             'recall': 24.503927378624955},\n",
      " 'strict': {'count': 30,\n",
      "            'f1_score': 21.193024541565734,\n",
      "            'precision': 21.65889355742297,\n",
      "            'recall': 21.043642280187274},\n",
      " 'type': {'count': 30,\n",
      "          'f1_score': 25.628949184555598,\n",
      "          'precision': 26.049980901451494,\n",
      "          'recall': 25.611223761382934}}\n"
     ]
    }
   ],
   "source": [
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1844-1871' # use your path\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1871-1879'\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1880-1889'\n",
    "path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1890-1905'\n",
    "\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work_misc\\\\csv\\\\spacy_medium_w_annotation.csv', converters={'spacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\spacy_medium_w_annotation.csv', converters={'spacy_medium': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "file_names = []\n",
    "\n",
    "for filename in all_files: \n",
    "    brev_filename = filename\n",
    "    brev_filename = re.match(r'.*\\d{4}-\\d{4}\\\\(.*).csv', brev_filename)\n",
    "    brev_filename = brev_filename.group(1)\n",
    "    \n",
    "    for i,r in csv_df.iterrows():\n",
    "        files = r['filename']\n",
    "        if files == brev_filename:\n",
    "            model = r['spacy_medium']\n",
    "            predictions.append(model)\n",
    "            f = r['filename']\n",
    "            file_names.append(f)\n",
    "            t = r['texts']\n",
    "            texts.append(t)\n",
    "            g = r['ground_truths']\n",
    "            ground_truths.append(g)\n",
    "    \n",
    "\n",
    "lists = {'filename': file_names, 'texts': texts, 'ground_truths': ground_truths, 'spacy_medium': predictions}\n",
    "\n",
    "df_dm = pd.DataFrame.from_dict(lists, orient = 'index')\n",
    "df_dm = df_dm.transpose()\n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "#x = muc.evaluate_all(predictions, ground_truths * 1, texts, verbose=True)\n",
    "pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n",
      "409\n",
      "{'exact': {'count': 30,\n",
      "           'f1_score': 22.36309785752006,\n",
      "           'precision': 22.90452635599695,\n",
      "           'recall': 22.158591646516097},\n",
      " 'partial': {'count': 30,\n",
      "             'f1_score': 24.641102914057722,\n",
      "             'precision': 25.158893557422967,\n",
      "             'recall': 24.503927378624955},\n",
      " 'strict': {'count': 30,\n",
      "            'f1_score': 21.193024541565734,\n",
      "            'precision': 21.65889355742297,\n",
      "            'recall': 21.043642280187274},\n",
      " 'type': {'count': 30,\n",
      "          'f1_score': 25.628949184555598,\n",
      "          'precision': 26.049980901451494,\n",
      "          'recall': 25.611223761382934}}\n"
     ]
    }
   ],
   "source": [
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1844-1871' # use your path\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1871-1879'\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1880-1889'\n",
    "path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1890-1905'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work_misc\\\\csv\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\spacy_large_w_annotation.csv', converters={'spacy_large': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "file_names = []\n",
    "\n",
    "for filename in all_files: \n",
    "    brev_filename = filename\n",
    "    brev_filename = re.match(r'.*\\d{4}-\\d{4}\\\\(.*).csv', brev_filename)\n",
    "    brev_filename = brev_filename.group(1)\n",
    "    \n",
    "    for i,r in csv_df.iterrows():\n",
    "        files = r['filename']\n",
    "        if files == brev_filename:\n",
    "            model = r['spacy_large']\n",
    "            predictions.append(model)\n",
    "            f = r['filename']\n",
    "            file_names.append(f)\n",
    "            t = r['texts']\n",
    "            texts.append(t)\n",
    "            g = r['ground_truths']\n",
    "            ground_truths.append(g)\n",
    "    \n",
    "\n",
    "lists = {'filename': file_names, 'texts': texts, 'ground_truths': ground_truths, 'spacy_large': predictions}\n",
    "\n",
    "df_dm = pd.DataFrame.from_dict(lists, orient = 'index')\n",
    "df_dm = df_dm.transpose()\n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "#x = muc.evaluate_all(predictions, ground_truths * 1, texts, verbose=True)\n",
    "pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n",
      "247\n",
      "{'exact': {'count': 30,\n",
      "           'f1_score': 22.36309785752006,\n",
      "           'precision': 22.90452635599695,\n",
      "           'recall': 22.158591646516097},\n",
      " 'partial': {'count': 30,\n",
      "             'f1_score': 24.641102914057722,\n",
      "             'precision': 25.158893557422967,\n",
      "             'recall': 24.503927378624955},\n",
      " 'strict': {'count': 30,\n",
      "            'f1_score': 21.193024541565734,\n",
      "            'precision': 21.65889355742297,\n",
      "            'recall': 21.043642280187274},\n",
      " 'type': {'count': 30,\n",
      "          'f1_score': 25.628949184555598,\n",
      "          'precision': 26.049980901451494,\n",
      "          'recall': 25.611223761382934}}\n"
     ]
    }
   ],
   "source": [
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1844-1871' # use your path\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1871-1879'\n",
    "#path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1880-1889'\n",
    "path = r'C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\annotations\\\\done\\\\1890-1905'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "#csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work_misc\\\\csv\\\\spacy_trf_w_annotation.csv', converters={'spacy_trf': literal_eval, 'ground_truths': literal_eval})\n",
    "csv_df = pd.read_csv('C:\\\\Users\\\\Sarah\\\\Desktop\\\\Bachelor\\\\Ibsen\\\\data_ibsen_work\\\\csv\\\\spacy_trf_w_annotation.csv', converters={'spacy_trf': literal_eval, 'ground_truths': literal_eval})\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "texts = []\n",
    "file_names = []\n",
    "\n",
    "for filename in all_files: \n",
    "    brev_filename = filename\n",
    "    brev_filename = re.match(r'.*\\d{4}-\\d{4}\\\\(.*).csv', brev_filename)\n",
    "    brev_filename = brev_filename.group(1)\n",
    "    \n",
    "    for i,r in csv_df.iterrows():\n",
    "        files = r['filenames']\n",
    "        if files == brev_filename:\n",
    "            model = r['spacy_trf']\n",
    "            predictions.append(model)\n",
    "            f = r['filenames']\n",
    "            file_names.append(f)\n",
    "            t = r['texts']\n",
    "            texts.append(t)\n",
    "            g = r['ground_truths']\n",
    "            ground_truths.append(g)\n",
    "    \n",
    "\n",
    "lists = {'filename': file_names, 'texts': texts, 'ground_truths': ground_truths, 'spacy_trf': predictions}\n",
    "\n",
    "df_dm = pd.DataFrame.from_dict(lists, orient = 'index')\n",
    "df_dm = df_dm.transpose()\n",
    "\n",
    "count = 0\n",
    "for e in ground_truths:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for e in predictions:\n",
    "    count += len(e) \n",
    "print(count)\n",
    "\n",
    "#x = muc.evaluate_all(predictions, ground_truths * 1, texts, verbose=True)\n",
    "pprint.pprint(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
